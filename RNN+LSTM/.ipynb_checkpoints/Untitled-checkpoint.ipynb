{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cấu trúc và nguyên lý hoạt động của RNN và LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neural Network?\n",
    "\n",
    "Neural Network là một thuật toán mô phỏng cách thức hoạt động của não bộ con người được thiết kế phát triển giúp phân loại, phát hiện các mẫu khác nhau. \n",
    "\n",
    "Nguyên tắc hoạt động của mạng nơ ron là sự lan truyền thông tin từ các noron này đến các noron khác. \n",
    "\n",
    "Tham khảo thêm tại: https://en.wikipedia.org/wiki/Neural_network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mạng nơ ron tích cập CNN\n",
    "\n",
    "Trong vài năm trở lại đây các thuật toán deep learning đã phát triển lên một cách mạnh mẽ do các yếu tố: dữ liệu, cấu hình thiết bị, nhu cầu. Việc phân loại các đối tượng (object) trở nên dễ dàng thậm chí có thể vượt qua cả con người nhờ cấu trúc đặc biệt của mạng noron. \n",
    "\n",
    "Nguyên lý: Sử dụng các kernel có kích thước khác nhau trượt qua input đầu vào để trích các thông tin khác nhau của đối tượng, từ đó năm bắt thông tin đối tượng thông qua các đặc trưng khác nhau nhờ các kernel khác nhau. Và áp dụng một hàm kích hoạt phi tuyến tính để chuẩn hóa dữ liệu đầu ra mong muốn.\n",
    "\n",
    "Tham khảo thêm: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "Vấn đề: Mạng CNN không thể nhớ các dữ liệu trước đó, nếu dữ liệu liên tục trong khoảng thời gian, chẳng hạn như một câu nói: Bầu trời nay đẹp quá, mình đi chơi đi. Trong một câu để năm bắt ngữ nghĩa của một câu văn thì ta cần biết chính xác các từ ngữ, ngữ nghĩa các từ trước nó. Chính vì vậy RNN ra đời đáp ứng nhu cầu trên phù hợp các bài toán dạng time series (dữ liệu liên tục theo thời gian).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN ( Recurent Neural Network)\n",
    "\n",
    "Không giống như CNN cho các tác vụ nhận dạng hình ảnh, chữ viết hay âm thanh. RNN có thể sử dụng trạng thái bên trong (bộ nhớ) của chúng để xử lý các chuỗi độ dài thay đổi của đầu vào. Các tính toán quá khứ sẽ áp dụng giúp cho suy luận phía sau. \n",
    "Nó tương tự như việc bạn gõ chữ \"thẳng\" các đề xuất phía sau là \"đứng\", \"hàng\". Đấy là cách thức mà RNN mô phỏng.\n",
    "\n",
    "Cấu trúc của RNN:\n",
    "\n",
    "![](images/RNN_structure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô tả toán học cấu trúc RNN: \n",
    "\n",
    "Đầu vào có dạng $X = [X_0, X_1, X_2, ...., X_n]$ \n",
    "\n",
    "Ví dụ: Hôm nay trời nắng to thì $X_0$ là vector đặc trưng cho từ \"hôm\", $X_1$ là vector đặc trưng cho từ \"nay\"....\n",
    "\n",
    "Tại thời điểm đàu tiên mạng sử dụng đầu vào là $X_0$ do chưa có trạng thái nào được lưu vào bộ nhớ trước đó.\n",
    "\n",
    "Mô tả toán học:\n",
    "\n",
    "Tại thời điểm t: input: $X_0$, output: $h_0$\n",
    "\n",
    "Sau đó $h_0$ được lưu lại trong bộ nhớ sử dụng cho lần dự đoán tiếp theo. Chính vì đó tại thời điểm t + 1\n",
    "\n",
    "input: $X_1$ và $h_0$, output: $h_1$\n",
    "\n",
    "Vòng lặp sẽ thực hiện tuần tự đến $X_n$ và đưa ra dự đoán theo yêu cầu bài toán. Bằng cách này RNN có thể ghi nhớ bối cảnh trong khi đào tạo. Đây là lí do cho RNN một nhược điểm:\n",
    "\n",
    "Kiến trúc của nó khá đơn giản nên khả năng liên kết các thành phần có khoảng cách xa trong câu không tốt. Vì thế, nếu bạn đang xử lý một đoạn văn dùng RNN, nó có thể bỏ qua những chi tiết ở đầu đoạn văn đó do bộ nhớ có hạn.\n",
    "\n",
    "Công thức tổng quát cho việc tính toán $h_t$\n",
    "\n",
    "#### $h_t = f( h_{t-1} , x_t)$\n",
    "\n",
    "Trong đó f(x, y) bên trên được tính toán bằng công thức bên dưới ( nó chính là những gì diễn ra bên trong khối A trong hình vẽ).\n",
    "\n",
    "#### g() = $f( h_{t-1} , x_t)$ = $W_{hh} * h_{t-1} + W_{xh} * x_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W là đại diện cho ma trận trọng số. h là vector ẩn đã được nhắc đến bên trên, còn $W_{hh}$ là trọng số ở trạng thái ẩn trước đó, $W_{hx}$ là trọng số ở trạng thái đầu vào hiện tại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tại mỗi trạng thái $h_t$ ta áp dụng một hàm kích hoạt phi tuyến tính tanh đưa dữ liệu về trong khoảng [-1, 1]. \n",
    "\n",
    "Lý do tại sao sử dụng hàm kích hoạt và giải thích hàm kích hoạt tham khảo thêm tại: https://blog.vietanhdev.com/posts/2019-09-23-cac-ham-kich-hoat-activation-function-trong-neural-networks/\n",
    "\n",
    "Một cách đơn giản để hiểu chức năng hàm kích hoạt là chuẩn hóa dữ liệu về một miền giá trị, giúp cho việc tính toán trên vi mạch hiệu quả hơn, cũng như giúp tránh việc các thâm số quá lớn không hiệu quả cho tính toán. \n",
    "\n",
    "Như vậy ta có cuối cùng \n",
    "\n",
    "#### $h_t =  tanh(g()) $\n",
    "\n",
    "Công thức hàm tanh ( trong môn cơ sở dữ liệu tri thức đã nhắc qua).\n",
    "\n",
    "Đầu ra của từng thời điểm là: chúng ta chỉ nên áp dụng đầu ra này tại thời điểm cuối cùng.\n",
    "\n",
    "#### $y_t = W_{hy} *  h_t$ \n",
    "\n",
    "Như vậy là hoàn thiện cơ bản về RNN rồi còn rất nhiều vấn đề cần khai thác với RNN. Ngoài ra có thể tham khảo thêm khóa học của \"đâng\" Andrew Ng tại diu-tu-be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vấn đề phụ thuộc dài hạn\n",
    "\n",
    "RNN có thể kết nối thông tin trước đó với tác vụ hiện tại, chẳng hạn như sử dụng các khung video trước đó có thể thông báo cho sự hiểu biết về khung hiện tại. Nếu RNN có thể làm điều này, chúng sẽ cực kỳ hữu ích. Nhưng họ có thể? Nó phụ thuộc.\n",
    "\n",
    "Đôi khi, chúng ta chỉ cần nhìn vào thông tin gần đây để thực hiện nhiệm vụ hiện tại. Ví dụ, hãy xem xét một mô hình ngôn ngữ đang cố gắng dự đoán từ tiếp theo dựa trên các từ trước đó. Nếu chúng ta đang cố gắng dự đoán từ cuối cùng trong các \"đám mây trên ...\", thì chúng ta không cần bất kỳ bối cảnh nào nữa rõ ràng là từ tiếp theo sẽ là bầu trời. Trong những trường hợp như vậy, khi khoảng cách giữa thông tin liên quan và địa điểm cần thiết là nhỏ, RNN có thể học cách sử dụng thông tin trong quá khứ.\n",
    "\n",
    "Nhưng cũng có những trường hợp chúng ta sử dụng nhiều bối cảnh hơn chẳng hạn như: \n",
    "\n",
    "\"Tôi là người Việt Nam và tôi yêu ...\" chúng ta cần xây dựng mô hình dự đoán cho từ tiếp theo của câu trên rõ ràng chúng ta phải năm bắt thông tin phía trước \"tôi là người Việt Nam\" từ đó suy ra \"tôi yêu đât nước mình\". Sự phụ thuộc ở đây trở nên phức tạp và các từ cũng ở xa vị trí dự đoán hơn, chính vì lẽ đó RNN hoạt động không hiệu quả khi phụ thuộc xa. Chính lẽ đó mà cái ông nào đó đã nghĩ ra LSTM đấy.\n",
    "\n",
    "Chúng ta phải biết ơn ông nào đã nghĩ ra LSTM nhé, nó rất vi diệu và cũng sẽ phức tạp hơn RNN rồi, điều đó cũng phải thôi, nó tốt hơn, khắc phục nhược điểm RNN mà. Nào let's go khai phá LSTM.\n",
    "\n",
    "### LSTM (Long Short Term Memory)\n",
    "\n",
    "Theo wiki media sẽ khá dài dòng như thế này:\n",
    "\n",
    "\"Bộ nhớ ngắn hạn ( LSTM ) là một kiến trúc mạng thần kinh tái phát (RNN) nhân tạo [1] được sử dụng trong lĩnh vực học tập sâu . Không giống như các mạng thần kinh feedforward tiêu chuẩn , LSTM có các kết nối phản hồi. Nó không chỉ có thể xử lý các điểm dữ liệu đơn lẻ (như hình ảnh) mà còn toàn bộ chuỗi dữ liệu (như lời nói hoặc video). Ví dụ, LSTM có thể áp dụng cho các tác vụ như không nhận dạng, nhận dạng chữ viết tay được kết nối [2] , nhận dạng giọng nói [3] [4] và phát hiện bất thường trong lưu lượng mạng hoặc IDS (hệ thống phát hiện xâm nhập).\"\n",
    "\n",
    "Thôi dẹp mẹ cái lí thuyết trên wiki đi vì nó khá khô khan và chán nản cho newbie và nhưng người oldbie haha. \n",
    "\n",
    "Nhưng thôi vẫn phải giới thiệu đôi dòng về LSTM mà đi gom nhặt được: \n",
    "\n",
    "\"Cái này đi nhặt được và cop về đây không phải tự viết nhé.\"\n",
    "\n",
    "\"Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\"\n",
    "\n",
    "* Cấu trúc LSTM\n",
    "\n",
    "Tất cả các biến thể của RNN đều có các module lặp lại. Trong RNN tiêu chuẩn chỉ có 1 layer là một hàm tanh tại mỗi module.\n",
    "LSTM sẽ có các module lặp lại tương tự RNN, tuy nhiên cấu trúc nó nhiều layer hơn chính xác chúng có đến 4 lớp.\n",
    "\n",
    "![](images/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kí hiệu hình vẽ trong sơ đồ:\n",
    "\n",
    "![](images/LSTM2-notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mô tả quá trình hoạt động theo sơ đồ\n",
    "\n",
    "Sự khác biệt LSTM và RNN\n",
    "\n",
    "1. \n",
    "Chìa khóa của LSTM đó là trạng thái của từng ô là đường thẳng chạy dọc theo khối module\n",
    "\n",
    "![](images/LSTM3-C-line.png)\n",
    "\n",
    "Nhiệm vụ của đường này nó sẽ nắm bắt toàn bộ các thông tin sự biến đổi sau mỗi lớp. \n",
    "\n",
    "\n",
    "2. LSTM có khả năng nhớ hoặc không nhớ các thông tin trước đó thông qua một cấu trúc đặc biệt gọi là cổng. \n",
    "\n",
    "Chẳng hạn: Hôm qua trời mưa to quá, hôm nay trời nắng sml. Rõ rảng trong 2 câu trên ngày hôm nay thời tiết thế nào không phụ thuộc vào ngày hôm qua, chính sẽ đó mà LSTM sẽ loại bỏ các thông tin thông cần thiết đó đi.\n",
    "\n",
    "Cấu trúc gọi là Gate để loại bỏ thông tin là: ( cấu trúc cổng quên)\n",
    "\n",
    "![](images/LSTM3-gate.png)\n",
    "\n",
    "Các thông tin khi đi qua lớp sigmoid (ô màu vàng) sẽ đưa dữ liệu về miền giá trị [0, 1] mô tả mức độ của mỗi thành phần sẽ được cho qua. Giá trị bằng 0 có nghĩa là không để bất cứ thứ gì qua, trong khi giá trị của một nghĩa là có thể cho phép mọi thứ thông qua! \n",
    "\n",
    "LSTM có chưa 3 cổng làm nhiệm vụ khác nhau chúng ta đi lần lượt bên dưới."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Từng bước LSTM \n",
    "\n",
    "+ B1: Bước đầu tiên của LSTM quyết định xem nên loại bỏ thông tin nào quyết định này được xác đinh thông qua cổng quên. \n",
    "\n",
    "![](images/LSTM3-focus-f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ B2: Bước tiếp theo xác định xem thông tin mới nào sẽ được lưu lại trong trạng thái tế bào.\n",
    "\n",
    "Điều này có 2 phần: \n",
    "\n",
    "    + Tạo ra dữ liệu mới từ đầu vào bằng việc sử dụng hàm tanh ( tương tự trong RNN)\n",
    "    \n",
    "    + Sử dụng một cổng sigmoide để xác định các giá trị nào sẽ được cập nhật.\n",
    "    \n",
    "![](images/LSTM3-focus-i.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ B3: Tiếp theo chúng ta sẽ kết hợp cả 2 quá trình loại bỏ dữ liệu và thêm dữ liệu nhằm tạo ra bản cập nhật trạng thái mới.\n",
    "\n",
    "![](images/LSTM3-focus-C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gía trị qua cổng quên là $f_t$ chúng ta nhân với trạng thái cũ là $C_{t_1}$ để loại bỏ thông tin.\n",
    "\n",
    "\n",
    "Ví dụ ta dễ hiểu hơn:\n",
    "\n",
    "Ta có trạng thái cũ là $C_{t-1} = [1, -3, -5, 4, 6, 2 , 1]$\n",
    "\n",
    "Ta có $f_t$ = [0, 1, 0.5, 0,2, 0.1, 0.4, 0.25]\n",
    "\n",
    "Thực hiện loại bỏ thông tin ta thấy: thông tin vị trí i = 0 đã bị loại bỏ, các thông tin khác sẽ bị loại bỏ đi ít nhiều tùy thuộc vào giá trị của vector $f_t$\n",
    "\n",
    "Trên thực tế trạng thái trông tin $C_{t-1}$ không đơn giản là một số như vậy nó có thể là một vector, ma trận...\n",
    "\n",
    "Việc thêm thông tin thực hiện bằng phép nhân $i_t*\\tilde{C}_t$.\n",
    "\n",
    "Việc lấy thêm thông tin nào nó tương tự như quá trình loại bỏ thông tin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ B4: Bước cuối cùng xác định đầu ra. \n",
    "\n",
    "Đầu ra này sẽ dựa trên trạng thái ô của chúng tôi, nhưng sẽ là phiên bản được lọc. Đầu tiên, chúng tôi chạy một lớp sigmoid quyết định phần nào của trạng thái tế bào chúng ta sẽ xuất ra. \n",
    "\n",
    "Sau đó các trạng thái tế bào được đưa qua hàm tanh ( đưa giá trị vè -1 đến 1) và nhân nó với đầu ra sigmoid để lọc thông tin đưa ra ouput khi đó chỉ các thành phần quyết định mới được đưa ra. \n",
    "\n",
    "![](images/LSTM3-focus-o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đoạn này cố mà đọc nhé.\n",
    "\n",
    "Đọc một hồi lý thuyết thì tóm tắt lại nội dung LSTM theo một ngôn ngữ dân dã là thế này:\n",
    "    \n",
    "Trước hết bạn thấy 2 cái giá trị C và h. Trong RNN chỉ có h là đàu ra trạng thái này sẽ là đàu vào của trạn thái khác. Nhưng ở LSTM lại khác. \n",
    "\n",
    "C: là tất cả giá trị ( mà người ta gọi là trạng thái tế bào) hay chính xác là các thông tin nó nhớ để cho qua trình học tiếp theo.\n",
    "\n",
    "Đơn giản nhé:\n",
    "\n",
    "Chúng ta sắp đi thi học kì chúng ta ôn một đống kiến thức: Cấu trúc dữ liệu, JAVA, WEB, MẠNG .... thì toàn bộ dữ liệu đó là C.\n",
    "\n",
    "Còn khi chúng ta kiểm tra gặp một bài toán bất kì thì chúng ta cần sử dụng từng kiến thức một để giải bài toán đó là h. Sẽ không có bài nào mà dùng một lúc tất cả các môn đâu. \n",
    "\n",
    "Hay chính xác là học thì lắm mà dùng có bao nhiêu, đọc cả quyển giáo trình đi thi vào có 1 chương à.\n",
    "\n",
    "Đấy là phân biệt C, h nhé.\n",
    "\n",
    "Bây giờ sẽ là bản tóm tắt LSTM.\n",
    "\n",
    "Lấy trường hợp ôn thi môn CSDL PHÂN TÁN ĐI ( thích môn này vl ra vì học éo ra gì mà đc A+). \n",
    "\n",
    "Thì có 6 chương, rồi nhắm mắt lại tưởng tượng quá trình LSTM là quá trình học đi quá lần lượt 6 chương đó, hahah. \n",
    "\n",
    "Ban đầu học chương 1 thì ta sẽ cần note được bao ý chính, bao kiến thức lưu mẹ vào C. và tổng kết sương sương là cái cần dùng vào h.\n",
    "\n",
    "Tiếp qua chương 2. Ta dùng kiến thức C, h trước đó để học thì một só cái kiến thức không cần bố mi loại mẹ nó đi, và dĩ nhiên chương mới thì lại thêm nó vào rồi, và bước lọc kết quả đó như kiểu học thì học đây sau 2 chương phải quên bớt đi chứ đấy là lọc đó.\n",
    "\n",
    "Vậy sau chương 2 ta được C, h mới quá trình quên cái bũ bớt đi, học thêm cái mới, và quên cách không ai muốn nói cho sang là vì éo dùng mấy nên nhỡ may quên đi thi mà vào thì đúng là bốc mắm.\n",
    "\n",
    "Lặp lại vậy sau 6 chương đi thi ( là quá trình đưa ra kết quả).\n",
    "\n",
    "Vâng LSTM là thế đó nó y như người mà haha.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kết luận\n",
    "\n",
    "Trên thực tế cấu trúc LSTM sẽ được tinh chế đi, đây là phiên bản tiêu chuẩn, sẽ có nhiều biến thể khác cải thiện khả năng nhớ của LSTM mà tôi thì rất ngại đọc nên xin phép không cho phần more đó vào. Nếu muốn bạn lên google search\" các biến thể LSTM\" chắc nó sẽ cho bạn kha khá kết quả /ms.\n",
    "\n",
    "Tôi thật biết ơn ai đã làm ra LSTM mà cuộc đời tôi chưa được làm project nào để mà giỏi lên LSTM được. Nên có dự án nào hái ra tiền nhớ gọi tui nhé.\n",
    "\n",
    "Liên hệ: Hoàng Mậu Trung, 0334 53 52 51\n",
    "\n",
    "Đến đây là hết rồi!. Đọc hiểu được sương sương vậy là tôi thấy tạn được rồi, khi gặp bài toán thực tế thì lại nghiên cứu tiếp. Rĩ nhiên nó có nhiều cái về LSTM nên mà sách mới 300 trang. Nhưng chung quy lại nó chỉ là 1 dòng dòng trong thư viện keras.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KHAI BÁO LSTM TRONG KERAS\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.layers.LSTM(units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
