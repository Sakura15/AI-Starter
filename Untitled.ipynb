{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cấu trúc và nguyên lý hoạt động của RNN và LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neural Network?\n",
    "\n",
    "Neural Network là một thuật toán mô phỏng cách thức hoạt động của não bộ con người được thiết kế phát triển giúp phân loại, phát hiện các mẫu khác nhau. \n",
    "\n",
    "Nguyên tắc hoạt động của mạng nơ ron là sự lan truyền thông tin từ các noron này đến các noron khác. \n",
    "\n",
    "Tham khảo thêm tại: https://en.wikipedia.org/wiki/Neural_network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mạng nơ ron tích cập CNN\n",
    "\n",
    "Trong vài năm trở lại đây các thuật toán deep learning đã phát triển lên một cách mạnh mẽ do các yếu tố: dữ liệu, cấu hình thiết bị, nhu cầu. Việc phân loại các đối tượng (object) trở nên dễ dàng thậm chí có thể vượt qua cả con người nhờ cấu trúc đặc biệt của mạng noron. \n",
    "\n",
    "Nguyên lý: Sử dụng các kernel có kích thước khác nhau trượt qua input đầu vào để trích các thông tin khác nhau của đối tượng, từ đó năm bắt thông tin đối tượng thông qua các đặc trưng khác nhau nhờ các kernel khác nhau. Và áp dụng một hàm kích hoạt phi tuyến tính để chuẩn hóa dữ liệu đầu ra mong muốn.\n",
    "\n",
    "Tham khảo thêm: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n",
    "\n",
    "Vấn đề: Mạng CNN không thể nhớ các dữ liệu trước đó, nếu dữ liệu liên tục trong khoảng thời gian, chẳng hạn như một câu nói: Bầu trời nay đẹp quá, mình đi chơi đi. Trong một câu để năm bắt ngữ nghĩa của một câu văn thì ta cần biết chính xác các từ ngữ, ngữ nghĩa các từ trước nó. Chính vì vậy RNN ra đời đáp ứng nhu cầu trên phù hợp các bài toán dạng time series (dữ liệu liên tục theo thời gian).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN ( Recurent Neural Network)\n",
    "\n",
    "Không giống như CNN cho các tác vụ nhận dạng hình ảnh, chữ viết hay âm thanh. RNN có thể sử dụng trạng thái bên trong (bộ nhớ) của chúng để xử lý các chuỗi độ dài thay đổi của đầu vào. Các tính toán quá khứ sẽ áp dụng giúp cho suy luận phía sau. \n",
    "Nó tương tự như việc bạn gõ chữ \"thẳng\" các đề xuất phía sau là \"đứng\", \"hàng\". Đấy là cách thức mà RNN mô phỏng.\n",
    "\n",
    "Cấu trúc của RNN:\n",
    "\n",
    "![](RNN_structure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô tả toán học cấu trúc RNN: \n",
    "\n",
    "Đầu vào có dạng $X = [X_0, X_1, X_2, ...., X_n]$ \n",
    "\n",
    "Ví dụ: Hôm nay trời nắng to thì $X_0$ là vector đặc trưng cho từ \"hôm\", $X_1$ là vector đặc trưng cho từ \"nay\"....\n",
    "\n",
    "Tại thời điểm đàu tiên mạng sử dụng đầu vào là $X_0$ do chưa có trạng thái nào được lưu vào bộ nhớ trước đó.\n",
    "\n",
    "Mô tả toán học:\n",
    "\n",
    "Tại thời điểm t: input: $X_0$, output: $h_0$\n",
    "\n",
    "Sau đó $h_0$ được lưu lại trong bộ nhớ sử dụng cho lần dự đoán tiếp theo. Chính vì đó tại thời điểm t + 1\n",
    "\n",
    "input: $X_1$ và $h_0$, output: $h_1$\n",
    "\n",
    "Vòng lặp sẽ thực hiện tuần tự đến $X_n$ và đưa ra dự đoán theo yêu cầu bài toán. Bằng cách này RNN có thể ghi nhớ bối cảnh trong khi đào tạo. Đây là lí do cho RNN một nhược điểm:\n",
    "\n",
    "Kiến trúc của nó khá đơn giản nên khả năng liên kết các thành phần có khoảng cách xa trong câu không tốt. Vì thế, nếu bạn đang xử lý một đoạn văn dùng RNN, nó có thể bỏ qua những chi tiết ở đầu đoạn văn đó do bộ nhớ có hạn.\n",
    "\n",
    "Công thức tổng quát cho việc tính toán $h_t$\n",
    "\n",
    "#### $h_t = f( h_{t-1} , x_t)$\n",
    "\n",
    "Trong đó f(x, y) bên trên được tính toán bằng công thức bên dưới ( nó chính là những gì diễn ra bên trong khối A trong hình vẽ).\n",
    "\n",
    "#### g() = $f( h_{t-1} , x_t)$ = $W_{hh} * h_{t-1} + W_{xh} * x_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W là đại diện cho ma trận trọng số. h là vector ẩn đã được nhắc đến bên trên, còn $W_{hh}$ là trọng số ở trạng thái ẩn trước đó, $W_{hx}$ là trọng số ở trạng thái đầu vào hiện tại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tại mỗi trạng thái $h_t$ ta áp dụng một hàm kích hoạt phi tuyến tính tanh đưa dữ liệu về trong khoảng [-1, 1]. \n",
    "\n",
    "Lý do tại sao sử dụng hàm kích hoạt và giải thích hàm kích hoạt tham khảo thêm tại: https://blog.vietanhdev.com/posts/2019-09-23-cac-ham-kich-hoat-activation-function-trong-neural-networks/\n",
    "\n",
    "Một cách đơn giản để hiểu chức năng hàm kích hoạt là chuẩn hóa dữ liệu về một miền giá trị, giúp cho việc tính toán trên vi mạch hiệu quả hơn, cũng như giúp tránh việc các thâm số quá lớn không hiệu quả cho tính toán. \n",
    "\n",
    "Như vậy ta có cuối cùng \n",
    "\n",
    "#### $h_t =  tanh(g()) $\n",
    "\n",
    "Công thức hàm tanh ( trong môn cơ sở dữ liệu tri thức đã nhắc qua).\n",
    "\n",
    "Đầu ra của từng thời điểm là: chúng ta chỉ nên áp dụng đầu ra này tại thời điểm cuối cùng.\n",
    "\n",
    "#### $y_t = W_{hy} *  h_t$ \n",
    "\n",
    "Như vậy là hoàn thiện cơ bản về RNN rồi còn rất nhiều vấn đề cần khai thác với RNN. Ngoài ra có thể tham khảo thêm khóa học của \"đâng\" Andrew Ng tại diu-tu-be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vấn đề phụ thuộc dài hạn\n",
    "\n",
    "RNN có thể kết nối thông tin trước đó với tác vụ hiện tại, chẳng hạn như sử dụng các khung video trước đó có thể thông báo cho sự hiểu biết về khung hiện tại. Nếu RNN có thể làm điều này, chúng sẽ cực kỳ hữu ích. Nhưng họ có thể? Nó phụ thuộc.\n",
    "\n",
    "Đôi khi, chúng ta chỉ cần nhìn vào thông tin gần đây để thực hiện nhiệm vụ hiện tại. Ví dụ, hãy xem xét một mô hình ngôn ngữ đang cố gắng dự đoán từ tiếp theo dựa trên các từ trước đó. Nếu chúng ta đang cố gắng dự đoán từ cuối cùng trong các \"đám mây trên ...\", thì chúng ta không cần bất kỳ bối cảnh nào nữa rõ ràng là từ tiếp theo sẽ là bầu trời. Trong những trường hợp như vậy, khi khoảng cách giữa thông tin liên quan và địa điểm cần thiết là nhỏ, RNN có thể học cách sử dụng thông tin trong quá khứ.\n",
    "\n",
    "Nhưng cũng có những trường hợp chúng ta sử dụng nhiều bối cảnh hơn chẳng hạn như: \n",
    "\n",
    "\"Tôi là người Việt Nam và tôi yêu ...\" chúng ta cần xây dựng mô hình dự đoán cho từ tiếp theo của câu trên rõ ràng chúng ta phải năm bắt thông tin phía trước \"tôi là người Việt Nam\" từ đó suy ra \"tôi yêu đât nước mình\". Sự phụ thuộc ở đây trở nên phức tạp và các từ cũng ở xa vị trí dự đoán hơn, chính vì lẽ đó RNN hoạt động không hiệu quả khi phụ thuộc xa. Chính lẽ đó mà cái ông nào đó đã nghĩ ra LSTM đấy.\n",
    "\n",
    "Chúng ta phải biết ơn ông nào đã nghĩ ra LSTM nhé, nó rất vi diệu và cũng sẽ phức tạp hơn RNN rồi, điều đó cũng phải thôi, nó tốt hơn, khắc phục nhược điểm RNN mà. Nào let's go khai phá LSTM.\n",
    "\n",
    "Theo wiki media sẽ khá dài dòng như thế này:\n",
    "\n",
    "\"Bộ nhớ ngắn hạn ( LSTM ) là một kiến trúc mạng thần kinh tái phát (RNN) nhân tạo [1] được sử dụng trong lĩnh vực học tập sâu . Không giống như các mạng thần kinh feedforward tiêu chuẩn , LSTM có các kết nối phản hồi. Nó không chỉ có thể xử lý các điểm dữ liệu đơn lẻ (như hình ảnh) mà còn toàn bộ chuỗi dữ liệu (như lời nói hoặc video). Ví dụ, LSTM có thể áp dụng cho các tác vụ như không nhận dạng, nhận dạng chữ viết tay được kết nối [2] , nhận dạng giọng nói [3] [4] và phát hiện bất thường trong lưu lượng mạng hoặc IDS (hệ thống phát hiện xâm nhập).\"\n",
    "\n",
    "Thôi dẹp mẹ cái lí thuyết trên wiki đi vì nó khá khô khan và chán nản cho newbie và nhưng người oldbie haha. \n",
    "\n",
    "Nhưng thôi vẫn phải giới thiệu đôi dòng về LSTM mà đi gom nhặt được: \n",
    "\n",
    "\"Cái này đi nhặt được và cop về đây không phải tự viết nhé.\"\n",
    "\n",
    "\"Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đợi nào rảnh update tiếp nhé!. \n",
    "\n",
    "# TU BI CON TI NIU ( LSTM KHÁ LÀ DÀI TẦM 2-3 TRANG A4 THÔI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
