{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Batch Normalization\n",
    "\n",
    "Trong một vài năm trở lại đây khi làm việc với mạng nơ-rơn nhân tạo thường bắt gặp một thuật ngữ là Batch Normalization. \n",
    "Vậy nó là gì, tác dụng của nó, cách sử dụng nó sao cho hiệu quả. \n",
    "\n",
    "Thứ nhất: Batch Normalization là một kĩ thuật trong các thuật toán machine learning và đặc biệt sử dụng nhiều trong mô hình mạng học sâu. \n",
    "\n",
    "Hiện tại nó được bổ sung vào một layer trong các mô hình học sâu, có thể bắt gặp như: XCeption, Inception, RestNet50...\n",
    "\n",
    "Cũng như tên gọi Batch Normalization giúp cho chuẩn hóa đầu vào để giảm thiểu ảnh hưởng của độ dốc không ổn định trong các mạng lưới thần kinh sâu. \n",
    "\n",
    "Điều này có nghĩa như thế nào??? Độ dốc ( hay cụ thể là gradient) thường thay đổi sau mỗi lần trainning, quá trình train-ning thì hàm loss nhỏ dần, khiến độ dốc nhỏ hơn. Trường hợp lý tưởng đồ thị độ dốc là đường con trơn. Tuy nhiên vì vài lí do nào đó quá trình tìm điểm tối ưu ( điểm hội tụ nó k thuật lợi như vậy) mà nó lại khá gập gềnh trông gai như biến cố vậy. Và khiến xảy ra các vấn đề overfitting, vashing gradient, explore gradient.... Khiến tốc độ đào tạo chậm đi, do nó bất ổn.\n",
    "\n",
    "![](https://ml.sotatek.com/assets/img/optimization-algorithms-1.png)\n",
    "\n",
    "\n",
    "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Train-and-Test-Set-Accuracy-of-Over-Training-Epochs-for-Deep-MLP-with-ReLU-with-15-Hidden-Layers.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do đó Batch Normalization là một kĩ thuật cũng giúp tăng tốc quá trình đào tạo. \n",
    "\n",
    "Batch Normalization gồm 2 thành phần:\n",
    "\n",
    "* Batch là lô: Do trong quá trình huấn luyện mô hình học máy thường huấn luyện trên 1 hoặc 1 số mẫu nhất định và đó gọi là lô. Chính vì vậy BN là kĩ thuật chuẩn hóa trên từng lô.\n",
    "\n",
    "* Normalization: chuẩn hóa\n",
    "\n",
    "Thông thường trong học máy, người ta thường chuẩn hóa dữ liệu đầu vào trước khi truyền dữ liệu đến lớp đầu vào. Lý do chúng tôi bình thường hóa là một phần để đảm bảo rằng mô hình của chúng tôi có thể khái quát hóa một cách thích hợp.\n",
    "\n",
    "BN là kĩ thuật thực thi chuẩn hóa dữ liệu nội bộ tức giữa các layer trong mạng thần kinh thay vì từ đầu vào. Chuẩn hóa nội bộ giới hạn sự thay đổi đồng biến thường xảy ra đối với các kích hoạt trong các lớp.\n",
    "\n",
    "Như đã đề cập trước đó, kỹ thuật BN hoạt động bằng cách thực hiện một loạt các thao tác trên dữ liệu đầu vào đi vào lớp BN. Dưới đây là một đoạn ký hiệu toán học của thuật toán BN trên một lô nhỏ.\n",
    "\n",
    "![](https://miro.medium.com/max/1288/1*jTPViestByum2ZZ3jXcmfw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mục địch chính của BN là đưa dữ liệu đàu vào về phân phối chuẩn có trung bình bằng 0 và phương sai là 1.\n",
    "\n",
    "BN gồm 4 bước trên mỗi lô:\n",
    "\n",
    "* Tính trung bình trên mỗi lô đầu vào \n",
    "* Tính phương sai bằng cách bình phương độ lệch chuẩn.\n",
    "* Chuẩn hóa các cầu vào để có tâm bằng 0 tức trung bình bằng 0. \n",
    "* Trong thao tác trên, giá trị trung bình của lô được trừ vào mỗi tức thời đầu vào. Sau đó chúng tôi chia kết quả theo giá trị căn bậc hai của phương sai và hằng số ( ε ).\n",
    "Thuật ngữ làm mịn ( ε ) đảm bảo sự ổn định bằng số trong hoạt động bằng cách dừng phân chia theo giá trị 0. Gía trị làm mịn thường là 0,00005.\n",
    "* Sau đó một phần bù được thêm vào điều này tạo ra tham số có thể học được trong quá trình đào tạo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lợi ích của việc BN\n",
    "+ Việc đưa kỹ thuật Batch bình thường hóa vào mạng lưới thần kinh sâu giúp cải thiện thời gian đào tạo\n",
    "+ BN cho phép sử dụng tỷ lệ học tập lớn hơn, thời gian hội tụ ngắn khi đào tạo mạng lưới thần kinh\n",
    "+ Giảm các vấn đề phổ biến của độ dốc biến mất\n",
    "+ Sự thay đổi đồng biến trong mạng lưới thần kinh bị giảm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một vài thuật ngữ trong bài còn xa lạ khó hiểu:\n",
    "    \n",
    "1. Độ biến biến mất  Vanishing gradient\n",
    "\n",
    "QUá trình khởi tạo trọng số mô hình quá thấp khiến cho việc loss và gradient nhỏ khiến cho việc cập nhật trọng số không còn có ý nghĩa trong quá trình huấn luyện. \n",
    "\n",
    "2. Thay đổi đồng biến \n",
    "\n",
    "Sự thay đổi đồng biến xảy ra khi phân phối các biến đầu vào khác nhau giữa tập dữ liệu huấn luyện và kiểm tra. Về mặt toán học, sự thay đổi đồng biến xảy ra nếu $P_{train} (X) # P_{test} (X) và  P_{train} (Y | X) = P_{test} (Y | X)$, đâu X là một tính năng.\n",
    "\n",
    "![](https://dkopczyk.quantee.co.uk/wp-content/uploads/2019/07/plot1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Đọc thêm các thuật ngữ khác taị: https://nttuan8.com/bai-10-cac-ky-thuat-co-ban-trong-deep-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
