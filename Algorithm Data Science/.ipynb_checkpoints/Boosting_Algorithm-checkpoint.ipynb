{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Boosting\n",
    "\n",
    "Phân tích bài phân loại email?\n",
    "\n",
    "Làm thế nào bạn có thể phân loại một email là SPAM hay không? Giống như mọi người khác, cách tiếp cận ban đầu của chúng tôi sẽ là xác định email 'thư rác' và 'không phải thư rác' bằng các tiêu chí sau. Nếu:\n",
    "\n",
    "+ Email chỉ có một tệp hình ảnh (hình ảnh quảng cáo), Đó là SPAM\n",
    "+ Email chỉ có (các) liên kết, Đó là một SPAM\n",
    "+ Phần thân email bao gồm câu như Bạn đã giành được tiền thưởng là $ xxxxxx, Đó là một SPAM\n",
    "+ Email từ tên miền chính thức của chúng tôi, Google Analyticsvidhya.com , không phải là SPAM\n",
    "+ Email từ nguồn đã biết, Không phải SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở trên, chúng tôi đã xác định nhiều quy tắc để phân loại email thành 'thư rác' hoặc 'không phải thư rác'. Nhưng, bạn có nghĩ rằng các quy tắc này đủ mạnh để phân loại thành công một email không? Không.\n",
    "\n",
    "Cá nhân, các quy tắc này không đủ mạnh để phân loại email thành 'thư rác' hoặc 'không phải thư rác'. Do đó, các quy tắc này được gọi là weak learner.\n",
    "\n",
    "To convert weak learner to strong learner, we’ll combine the prediction of each weak learner using methods like:\n",
    "\n",
    "* Using average/ weighted average\n",
    "* Considering prediction has higher vote\n",
    "\n",
    "( dịch sang TA hơn thô nên thôi để vậy cho chuẩn nhất của phương pháp).\n",
    "\n",
    "Ví dụ: Ở trên, chúng tôi đã xác định 5 người học yếu. Trong số 5, 3 được bình chọn là 'SPAM' và 2 được bình chọn là 'Không phải SPAM'. Trong trường hợp này, theo mặc định, chúng tôi sẽ coi email là SPAM vì chúng tôi có (3) phiếu bầu cao hơn cho 'SPAM'.\n",
    "\n",
    "##### Vậy nên: Definition: The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ý tưởng: thay vì xây dựng một mô hình dự đoán (chẳng hạn descision tree) có độ chính xác tương đối, ta đi xây dựng nhiều mô hình dự đoán có độ chính xác kém hơn (weak learner) khi đi riêng lẻ nhưng lại cho độ chính xác cao khi kết hợp lại.\n",
    "\n",
    "Ta có thể hình dung mỗi weak learner gồm học sinh yếu, khá, giỏi và thầy giáo. Trong đó, trọng số uy tín về kiến thức của thầy giáo sẽ là cao nhất và học sinh yếu sẽ là thấp nhất. Khi bạn đặt câu hỏi nào đó và cần những người này đưa ra kết luận, nếu nhiều người cùng có chung kết luận hoặc uy tín của những người đưa ra kết luận cao hơn tập thể thì ta có thể tin kết luận này là đúng.\n",
    "\n",
    "![](images/boots.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thuật toán triển khai Boots: AdaBoost\n",
    "\n",
    "AdaBoost là một thuật toán Boosting cụ thể được phát triển cho các vấn đề phân loại. \n",
    "\n",
    "Đầu vào: X [n samples x d features].\n",
    "\n",
    "Đầu ra : Y [n samples x k classes].\n",
    "\n",
    "Ma trận trọng số: W [n x 1 ].\n",
    "\n",
    "Điểm yếu (một bộ phân loại yếu) được xác định bởi tỉ lệ lỗi theo công thức:\n",
    "\n",
    "![](images/error.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mỗi lần lặp, AdaBoost xác định các điểm dữ liệu được phân loại sai, tăng trọng số của chúng (và giảm trọng số của các điểm chính xác, theo nghĩa nào đó) để phân loại tiếp theo sẽ chú ý thêm để có được chúng đúng. \n",
    "\n",
    "Mô tả trong hình ảnh bên dưới\n",
    "\n",
    "![](images/adaboost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ với điểm yếu được xác định, bước tiếp theo là tìm ra cách kết hợp chuỗi các mô hình để làm cho đoàn thể mạnh hơn theo thời gian.\n",
    "\n",
    "#### Diễn giải thuật toán:\n",
    "\n",
    "\n",
    "\n",
    "* Bước 1: Khởi tạo trọng số W : W1, W2, W3 ,..., Wn = 1/n\n",
    "    \n",
    "* Bước 2: Lặp lại lần lượt từng điểm yếu: For i in [1, M]: ( các điểm yếu số bộ phân loại yếu)\n",
    "    \n",
    "    Bước 2.1: Tính toán lỗi error_i (theo công thức bên trên)\n",
    "    \n",
    "    Bước 2.2: Tính hệ số tương quan coefficient: a = $log(\\frac{1-err}{err}) + log(K-1)$\n",
    "\n",
    "    Bước 2.3: Cập nhật Wj với Wj $\\in$ W. Ta có Wj =  $W_j *  e  ^ {a *  I(C_i(X_j)}$\n",
    "    \n",
    "        I(...) = 1 nếu phân loại đung, 0 nếu phân loại sai\n",
    "        \n",
    "        Phân loại sai thì tăng weight dự đoán ra lớp đó lên.\n",
    "    \n",
    "    \n",
    "    Bước 2.4: Chuẩn hóa trọng số: W = W - mean(W)\n",
    "\n",
    "* Bước 3: Tìm xem với các bộ phân loại yếu đó thì xem xem classes nào được vote nhiều nhất là đầu ra \n",
    "    \n",
    "Thực hiện trong python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X,Y = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
    "                          n_redundant=0, n_repeated=0, random_state=102)\n",
    "clf = AdaBoostClassifier(n_estimators=4, random_state=0, algorithm='SAMME')\n",
    "clf.fit(X, Y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thuật toán triển khai Boots: Gradient Boosting (GBM)\n",
    "\n",
    "Gradient boosting tiếp cận vấn đề một chút khác nhau. Thay vì điều chỉnh trọng số của các điểm dữ liệu, Gradient boosting tập trung vào sự khác biệt giữa dự đoán và sự thật cơ bản.\n",
    "\n",
    "\n",
    "Một bộ phân loại yếu ( 1 điểm yếu được xác định)\n",
    "\n",
    "![](images/gboots.png)\n",
    "\n",
    "#### Diễn giải thuật toán \n",
    "\n",
    "* Bước 1: Fit estimators F\n",
    "\n",
    "* Bước 2: Lặp qua từng bộ phân loại yếu for i in [1, M]:\n",
    "    \n",
    "    Bước 2.1: Tính lỗi gây từ bộ phân loại yếu ( theo hàm loss được định nghĩa trong hình trên)\n",
    "    \n",
    "    Bước 2.2: Tính toán đạo hàm thành phần: hàm loss theo từng biến đầu vào \n",
    "    \n",
    "    Trong Gradient Boosting, chúng tôi đang kết hợp các dự đoán của nhiều mô hình , vì vậy chúng tôi không tối ưu hóa trực  tiếp các tham số mô hình mà là các dự đoán mô hình được tăng cường.\n",
    "    \n",
    "    Bước 2.3 Tổng hợp đạo hàm thành phần thành vector cùng chiều vs đầu ra $ h^i $\n",
    "    \n",
    "    Bước 2.4 Cải thiện kết quả dự đoán và kết quả này dùng để fit cho mô hình tiếp theo\n",
    "        \n",
    "    F(X)_new = F(X) +   $ lr * h^i $\n",
    "    Đầu ra của cây thứ 2 sẽ được fit với F(X)_new này nhé!\n",
    "\n",
    "\n",
    "![](images/GBM.jpg)\n",
    "Thực hiện trong Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(n_estimators=3,learning_rate=1)\n",
    "model.fit(X,Y)\n",
    "\n",
    "# for classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thuật toán dựa trên Boots: XGBoost (Extreme Gradient Boosting).\n",
    "\n",
    "XGBoost là viết tắt của Extreme Gradient Boosting; đó là một triển khai cụ thể của phương pháp Gradient Boosting sử dụng các xấp xỉ chính xác hơn để tìm mô hình cây tốt nhất.\n",
    "\n",
    "Đây là thuật toán chiến thắng nhiều nhất tại các thử thách của Kangle. \n",
    "\n",
    "Phương pháp GBM bên trên sử dụng đạo hàm tuyệt đối để tính toán đạo hàm thành phần từ hàm loss. \n",
    "\n",
    "Sự khác biệt giữa GBM và XGBoost (cũng là điểm đáng giá nhất)\n",
    "\n",
    "* Tính toán độ dốc bậc hai, tức là đạo hàm riêng thứ hai của hàm mất ( sử dụng đạo hàm bậc 2)\n",
    "\n",
    "* Chính quy hóa nâng cao (L1 & L2), giúp cải thiện khái quát hóa mô hình.\n",
    "\n",
    "* Tính toán song song \n",
    "\n",
    "* Đối với GBM tiêu chí dừng xác định thông qua max_depth  thì XGboost thực hiện cắt tỉa cây.\n",
    "\n",
    "* Quản lý tài nguyên tốt hơn\n",
    "\n",
    "Do đó: XGBoost có các lợi thế bổ sung: đào tạo rất nhanh và có thể được song song / phân phối trên các cụm\n",
    "\n",
    "Bảng so sánh hiệu năng/thời gian\n",
    "![](images/XGBoost.jpeg)\n",
    "\n",
    "Triển khai trong Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "'''Bây giờ bạn sẽ chuyển đổi tập dữ liệu thành một cấu trúc dữ liệu được tối ưu hóa được gọi là DmatrixXGBoost \n",
    "hỗ trợ và mang lại cho nó hiệu suất và hiệu quả đạt được. Bạn sẽ sử dụng điều này sau trong hướng dẫn.\n",
    "'''\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các siêu tham số quan tâm trong mô hình xgboost\n",
    "\n",
    "* learning_rate\n",
    "\n",
    "* max_depth: độ sâu từng cây \n",
    "\n",
    "* subsample: tỷ lệ phần trăm mẫu được sử dụng trên mỗi cây. Giá trị thấp có thể dẫn đến thiếu.\n",
    "* colsample_bytree: tỷ lệ phần trăm của các tính năng được sử dụng trên mỗi cây. Giá trị cao có thể dẫn đến quá mức.\n",
    "* n_estimators: số lượng cây bạn muốn xây dựng.\n",
    "* objective: xác định hàm mất được sử dụng như reg:linearđối với các vấn đề hồi quy, reg:logisticđối với các vấn đề phân loại chỉ có quyết định, binary:logisticđối với các vấn đề phân loại có xác suất.\n",
    "\n",
    "* gamma: kiểm soát xem một nút đã cho sẽ phân tách dựa trên mức giảm tổn thất dự kiến ​​sau khi phân tách. Giá trị cao hơn dẫn đến chia nhỏ hơn. Chỉ hỗ trợ cho người học dựa trên cây.\n",
    "* alpha: L1 chính quy trên trọng lượng lá. Một giá trị lớn dẫn đến thường xuyên hơn.\n",
    "* lambda: Chuẩn hóa L2 trên trọng lượng lá và mượt hơn so với chuẩn hóa L1.\n",
    "\n",
    "Do việc chuẩn hóa L1 trong GBDT được áp dụng cho điểm số lá thay vì trực tiếp cho các tính năng như trong hồi quy logistic, nó thực sự phục vụ để giảm độ sâu của cây. Điều này đến lượt nó sẽ có xu hướng làm giảm tác động của các tính năng ít dự đoán hơn, nhưng nó không quá ấn tượng vì về cơ bản là loại bỏ tính năng này, như xảy ra trong hồi quy logistic. Bạn có thể nghĩ về việc chuẩn hóa L1 mạnh hơn đối với các tính năng ít dự đoán hơn so với chính quy L2. Nhưng sau đó, có thể có ý nghĩa khi sử dụng cả hai: một số L1 để trừng phạt các tính năng ít dự đoán hơn, nhưng sau đó cũng có một số L2 để trừng phạt thêm điểm số lá lớn mà không quá khắc nghiệt đối với các tính năng ít dự đoán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "\n",
    "# for classification\n",
    "xg_reg = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "\n",
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cải thiện mô hình XGBoost bằng k-fold Cross Validation\n",
    "\n",
    "K-fold cross validation là gì?\n",
    "\n",
    "Cross validation là một phương pháp thống kê được sử dụng để ước lượng hiệu quả của các mô hình học máy. Nó thường được sử dụng để so sánh và chọn ra mô hình tốt nhất cho một bài toán. \n",
    "\n",
    "Bước thực hiện:\n",
    "\n",
    "* B1: Xáo trộn dataset một cách ngẫu nhiên\n",
    "* B2: Chia dataset thành k nhóm\n",
    "* B3 Với mỗi nhóm:\n",
    "\n",
    "    + B3.1 Sử dụng nhóm hiện tại để đánh giá hiệu quả mô hình\n",
    "    \n",
    "    + B3.2 Các nhóm còn lại được sử dụng để huấn luyện mô hình\n",
    "    \n",
    "    + B3.3 Huấn luyện mô hình\n",
    "    \n",
    "    + B3.4 Đánh giá và sau đó hủy mô hình\n",
    "    \n",
    "* B4 Tổng hợp hiệu quả của mô hình dựa từ các số liệu đánh giá\n",
    "\n",
    "Triển khai trong Python với thuật toán XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# cv_results chứa các số liệu RMSE đào tạo và thử nghiệm cho mỗi vòng tăng tốc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các tham số:\n",
    "    \n",
    "* num_boost_round: biểu thị số lượng cây bạn xây dựng (tương tự n_estimators)\n",
    "    \n",
    "* metrics: cho biết các số liệu đánh giá sẽ được xem trong CV\n",
    "* as_pandas: để trả về kết quả trong DataFrame của pandas.\n",
    "* early_stopping_rounds: hoàn thành việc đào tạo mô hình sớm nếu số liệu giữ (\"rmse\" trong trường hợp của chúng tôi) không cải thiện cho một số vòng nhất định.\n",
    "* seed: cho độ tái lập của kết quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem các bước mà thuật toán thực hiện bằng hình ảnh:\n",
    "    \n",
    "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)   \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgb.plot_tree(xg_reg,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/output_54_0_izbh1l.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chúng ta kiểm tra tầm quan trọng của từng tính năng: Từ đó ta có thể lựa chọn tính năng hehehe\n",
    "\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/output_57_0_s5agpl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thuật toán triển khai Boosting: Light GBM (hàng của mai cờ xô sốp nhé) \n",
    "\n",
    "Đây là một thuật toán nằm trong nhóm thuật toán Boosting hiện tại cạnh tranh khá gay gắt với XGBoost. Tuy nhiên đánh giá 2 thuật toán 1 9 1 10 hiện tại thì XGBoost là 10 còn Light GBM là 9.\n",
    "\n",
    "Light GBM là gì? Và nó hoạt động ra sao?\n",
    "\n",
    "Nó là gì thì như dòng giới thiệu bên trên.\n",
    "\n",
    "Nó hoạt động ra sao thì sẽ như giải thích bên dưới.\n",
    "\n",
    "Cùng nhau nhớ lại nhé, với tất cả các cây trong nhóm thuật toán boosting thì đều phát triển cây theo chiều ngang(chiều rộng), còn Light GBM áp dụng việc phát triển cây theo chiều dọc(chiều sâu). Khi phát triển từng lá nó tìm lá có tổn thất lớn nhất để phát triển tiếp. \n",
    "\n",
    "Nguyên lí hoạt động của việc phát triển cây này như 2 thuật toán tìm kiếm theo chiều sâu và rộng. \n",
    "\n",
    "Với Light GBM là chiều sâu, XGBoost là chiều rộng.\n",
    "\n",
    "Còn việc áp dụng cập nhật và chọn lá vvv như XG Boost.\n",
    "![](images/LightGBM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Đôi nét khen Light GBM:\n",
    "\n",
    "Kích thước của dữ liệu đang tăng lên từng ngày và nó trở nên khó khăn cho các thuật toán khoa học dữ liệu truyền thống để đưa ra kết quả nhanh hơn. Light GBM có tiền tố là 'Light' vì tốc độ cao. Light GBM có thể xử lý kích thước lớn của dữ liệu và chiếm bộ nhớ thấp hơn để chạy . Một lý do khác khiến Light GBM trở nên phổ biến là vì nó tập trung vào độ chính xác của kết quả . LGBM cũng hỗ trợ học GPU và do đó các nhà khoa học dữ liệu đang sử dụng rộng rãi LGBM để phát triển ứng dụng khoa học dữ liệu.\n",
    "\n",
    "Light GBM rất nhạy cảm với việc quá mức và có thể dễ dàng vượt qua dữ liệu nhỏ. Chúng không có ngưỡng về số lượng hàng nhưng kinh nghiệm của tôi cho thấy tôi chỉ sử dụng nó cho dữ liệu có hơn 10.000 hàng.\n",
    "\n",
    "Thực hiện Light GBM với Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 10\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 10\n",
    "clf = lgb.train(params, d_train, 100)\n",
    "\n",
    "#Prediction\n",
    "y_pred=clf.predict(x_test)\n",
    "#convert into binary values\n",
    "for i in range(0,99):\n",
    "    if y_pred[i]>=.5:       # setting threshold to .5\n",
    "        y_pred[i]=1\n",
    "    else:  \n",
    "        y_pred[i]=0\n",
    "        \n",
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for regression\n",
    "\n",
    "lgbm= lightgbm.LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, \n",
    "                       subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, \n",
    "                       min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n",
    "                       colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, \n",
    "                       silent=True, importance_type='split', **kwargs)\n",
    "\n",
    "#for classification\n",
    "\n",
    "lgbm = lightgbm.LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, \n",
    "                        subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, \n",
    "                        min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n",
    "                        colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1, \n",
    "                        silent=True, importance_type='split', **kwargs)\n",
    "# for ranker\n",
    "\n",
    "lgbm = lightgbm.LGBMRanker(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, \n",
    "                           subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, \n",
    "                           min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n",
    "                           colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=-1,\n",
    "                           silent=True, importance_type='split', **kwargs)\n",
    "\n",
    "clf = lgbm.fit(X, y, sample_weight=None, init_score=None, eval_set=None, eval_names=None, eval_sample_weight=None, \n",
    "    eval_class_weight=None, eval_init_score=None, eval_metric=None, early_stopping_rounds=None, verbose=True, \n",
    "    feature_name='auto', categorical_feature='auto', callbacks=None, init_model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khám phá các tham số:\n",
    "    \n",
    "* boosting: thuật toán sử dụng xây dựng cây: gbdt cây quyết định, rf: cây ngẫu nhiên\n",
    "            \n",
    "* num_leaves: lá cây tối đa ( quá lơn sẽ gây overfitting) < 2 ^ depth\n",
    "\n",
    "* max_depth: độ sâu của cây <=0 có nghĩa là không giới hạn quá lớn cũng gây overfitting\n",
    "\n",
    "* learning_rate: tỉ lệ đào tạo lớn thì học nhanh, chậm thì học lâu \n",
    "\n",
    "* n_estimators: số cây muốn tạo ra\n",
    "\n",
    "* subsample_for_bin: Số lượng mẫu để xây dựng thùng\n",
    "\n",
    "* metric: \n",
    "    + mae: mean absolute error\n",
    "    + mse: mean squared error\n",
    "    + binary_logloss: loss for binary classification\n",
    "    + multi_logloss: loss for multi classification\n",
    "\n",
    "vvv có thể đọc thêm trên mạng nhé nó quá nhiều tham số "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
